{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQbTE0qZDUhMJwJvcDAD+f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mobikhani/NLP-Assignment-/blob/main/NLP_ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Choose any corpus of your choice of at least 200 MBs of any domain in NLP and perform\n",
        "the following tasks:\n",
        "\n",
        "• Text Preprocessing (Text Cleaning, Stemming / Lemmatization)\n",
        "\n",
        "• Word Embedding (using an algorithm like Word2Vec, Glove, FastText)\n",
        "\n",
        "• Encoding Techniques (Bag of Words, One – Hot)\n",
        "\n",
        "• Parts of Speech tagging."
      ],
      "metadata": {
        "id": "YDcCobj_QiLV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0AGFN7uC_Fe",
        "outputId": "8ba669a2-64e7-4747-b6ba-2b4f0e00820c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the file path (make sure this is the correct path in your drive)\n",
        "file_path = '/content/drive/MyDrive/arxiv_papers.csv'\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(file_path):\n",
        "    print(\"File found and loaded!\")\n",
        "else:\n",
        "    print(\"File not found. Please check the path.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r2A3fhGEbrn",
        "outputId": "f0badf46-2495-4fb6-9c46-34cb4ba8a503"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File found and loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/arxiv_papers.csv' # Removed extra spaces at the beginning of this line\n",
        "df = pd.read_csv(file_path) # Removed extra spaces at the beginning of this line\n",
        "print(df.columns)  # Removed extra spaces at the beginning of this line\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EZR7FsmHjBp",
        "outputId": "44373305-760d-48c5-abda-22484e1e8ce1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['abstract', 'author', 'date', 'pdf_url', 'title', 'pdf_text'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (assuming it's a CSV)\n",
        "file_path = '/content/drive/MyDrive/arxiv_papers.csv'  # Adjust file path\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Check the columns and data types\n",
        "print(df.columns)\n",
        "\n",
        "# Select the 'abstract' or 'pdf_text' column for NLP tasks\n",
        "texts = df['abstract']  # or df['pdf_text'] if that's more relevant\n",
        "\n",
        "print(texts.head())  # Preview the first few entries\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJED7TSdIa-K",
        "outputId": "f5037d8d-b95b-4b65-e31f-bd1f8fe01dec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['abstract', 'author', 'date', 'pdf_url', 'title', 'pdf_text'], dtype='object')\n",
            "0    We first present our view of detection and cor...\n",
            "1    We first present our view of detection and cor...\n",
            "2    The choice of modeling units is critical to au...\n",
            "3    Why should computers interpret language increm...\n",
            "4    Stance detection is a classification problem i...\n",
            "Name: abstract, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/arxiv_papers.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKDDtVJZM1yK",
        "outputId": "a6915275-8f3f-4afe-df03-deac58f6e6d0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/arxiv_papers.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Text Preprocessing (Text Cleaning, Tokenization, Lemmatization)"
      ],
      "metadata": {
        "id": "aNGZr75lQYh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Remove non-alphabetic characters and convert to lowercase\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
        "\n",
        "    # Tokenize the cleaned text\n",
        "    tokens = word_tokenize(cleaned_text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "    return lemmatized_words\n",
        "\n",
        "# Apply preprocessing to all abstracts\n",
        "df['processed_text'] = df['abstract'].apply(preprocess_text)  # or df['pdf_text']\n",
        "print(df['processed_text'].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tyY7rMMN1iU",
        "outputId": "d2217be1-fe01-4f02-f678-1ed49de5c452"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [first, present, view, detection, correction, ...\n",
            "1    [first, present, view, detection, correction, ...\n",
            "2    [choice, modeling, unit, critical, automatic, ...\n",
            "3    [computer, interpret, language, incrementally,...\n",
            "4    [stance, detection, classification, problem, n...\n",
            "Name: processed_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4lM4muPrQewB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Word Embedding (Using Word2Vec)"
      ],
      "metadata": {
        "id": "kWMyQeb7QNFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Prepare the data for Word2Vec (list of token lists)\n",
        "text_list = df['processed_text'].tolist()\n",
        "\n",
        "# Create the Word2Vec model\n",
        "word2vec_model = Word2Vec(text_list, vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "# Save the model\n",
        "word2vec_model.save(\"word2vec.model\")\n",
        "\n",
        "# Example: Find similar words to 'data'\n",
        "print(word2vec_model.wv.most_similar('data'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAC0rNx0ObPE",
        "outputId": "4d7f5552-965f-43e8-cb58-80f238b6b01c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('corpus', 0.5758063197135925), ('labeled', 0.5582237839698792), ('sample', 0.5454695224761963), ('indomain', 0.5227629542350769), ('unlabeled', 0.5107916593551636), ('resource', 0.5081765055656433), ('amount', 0.5058395266532898), ('scarce', 0.5009562969207764), ('qamrs', 0.4915483891963959), ('testing', 0.49047186970710754)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Encoding Techniques (Bag of Words and One-Hot Encoding)\n",
        "\n",
        "(i)Bag of Words (BoW):\n"
      ],
      "metadata": {
        "id": "457eo2EsP423"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Convert processed text back to strings for BoW\n",
        "lemmatized_texts = [' '.join(text) for text in df['processed_text']]\n",
        "\n",
        "# Create a Bag of Words representation\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(lemmatized_texts)\n",
        "\n",
        "# Display the BoW matrix and feature names\n",
        "print(X_bow.toarray())\n",
        "print(vectorizer.get_feature_names_out()[:20])  # Show first 20 feature names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Hv-pA9rPy8U",
        "outputId": "b9a93843-b0f5-4134-b258-8316c8da7e03"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "['aa' 'aaai' 'aac' 'aadit' 'aae' 'aaelike' 'aalstm' 'aalto' 'aam' 'aan'\n",
            " 'aapr' 'aardvark' 'aarnethompsonuther' 'aat' 'ab' 'abacha' 'abandon'\n",
            " 'abandoned' 'abandoning' 'abater']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(ii)One-Hot Encoding:"
      ],
      "metadata": {
        "id": "JZC0yjfURbQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Flatten all tokens and extract unique words from the processed texts\n",
        "unique_words = list(set([word for text in df['processed_text'] for word in text]))\n",
        "\n",
        "# Reshape for OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "one_hot_encoded = one_hot_encoder.fit_transform(np.array(unique_words).reshape(-1, 1))\n",
        "\n",
        "print(one_hot_encoded)  # Display the one-hot encoded matrix\n"
      ],
      "metadata": {
        "id": "LGRRrVwmS4Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Parts of Speech (POS) Tagging"
      ],
      "metadata": {
        "id": "rgPR0dYrVxS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Function for POS tagging\n",
        "def pos_tagging(text):\n",
        "    return nltk.pos_tag(text)\n",
        "\n",
        "# Apply POS tagging to the processed text\n",
        "df['pos_tags'] = df['processed_text'].apply(pos_tagging)\n",
        "\n",
        "# Display POS tags for the first row\n",
        "print(df['pos_tags'].head())\n"
      ],
      "metadata": {
        "id": "zTz3QDsQVnMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Basic NLP Tasks\n",
        "For the second part, we’ll choose Named Entity Recognition (NER) and Topic Modeling (LDA)."
      ],
      "metadata": {
        "id": "f0pbLrP0XKCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Named Entity Recognition (NER)\n",
        "We’ll use spaCy to extract named entities from each abstract or pdf_text."
      ],
      "metadata": {
        "id": "9Ecb4q7OXsXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Download spaCy's small English model\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to perform Named Entity Recognition\n",
        "def ner(text):\n",
        "    doc = nlp(' '.join(text))  # Join processed tokens back into text\n",
        "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "# Apply NER to the processed text\n",
        "df['entities'] = df['processed_text'].apply(ner)\n",
        "\n",
        "# Display named entities for the first row\n",
        "print(df['entities'].head())\n"
      ],
      "metadata": {
        "id": "bqEXj8BuXYK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Topic Modeling (LDA)\n",
        "We’ll use Gensim to perform topic modeling using\n",
        "\n",
        " Latent Dirichlet Allocation (LDA)."
      ],
      "metadata": {
        "id": "sEIifUZmYKuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Create a dictionary and corpus for LDA\n",
        "dictionary = corpora.Dictionary(df['processed_text'])\n",
        "corpus = [dictionary.doc2bow(text) for text in df['processed_text']]\n",
        "\n",
        "# Train the LDA model\n",
        "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
        "\n",
        "# Print the topics\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ],
      "metadata": {
        "id": "u_j6pzQVYV46"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}